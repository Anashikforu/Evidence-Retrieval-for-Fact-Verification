{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import *\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "folder_path = 'articles_all'\n",
    "file_name = \"collated_data.txt\"\n",
    "inverted_index_file = \"inverted_index.txt\"\n",
    "file_contents_list = []\n",
    "\n",
    "\n",
    "file_name_number_mapping_dict = {}\n",
    "number_file_name_mapping_dict = {}\n",
    "inverted_index_intermediate = {}\n",
    "inverted_index = {}\n",
    "num_docs = 1\n",
    "idf_dict = {}\n",
    "tf_dict = {}\n",
    "file_to_vec_dict = {}\n",
    "query_to_vec_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fileContents:\n",
    "    file_name = \"\"\n",
    "    file_number = 0\n",
    "    contents_raw = \"\"\n",
    "    contents_tokens = []\n",
    "\n",
    "class sentenceMatch:\n",
    "    file_name = 0\n",
    "    match = 0\n",
    "    sentence_raw = \"\"\n",
    "    sentence_tokenised = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileOnly(file_num):\n",
    "\n",
    "    for entry in file_contents_list:\n",
    "        if entry.file_number == int(file_num):\n",
    "            file_name = entry.file_name\n",
    "\n",
    "    # file_name = number_file_name_mapping_dict[str(file_num)]\n",
    "    file_path = folder_path + '/' + file_name\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = re.sub(\"\\\\n\", \"\", data)\n",
    "    data = re.sub(\"'''\", \"\", data)\n",
    "    data = re.sub(\"''\", \"\", data)\n",
    "    data = re.sub(\",\", \"\", data)\n",
    "\n",
    "    data_list = data.split('.')\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "def tokenize_and_remove_punctuations(s):\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    modified_string = s.translate(translator)\n",
    "    modified_string = ''.join([i for i in modified_string if not i.isdigit()])\n",
    "    return nltk.word_tokenize(modified_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words = [word for word in open('stopwords.txt','r').read().split('\\n')]\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(contents):\n",
    "    contents = contents.lower()\n",
    "    title_start = contents.find('<title>')\n",
    "    title_end = contents.find('</title>')\n",
    "    title = contents[title_start+len('<title>'):title_end]\n",
    "    text_start = contents.find('<text>')\n",
    "    text_end = contents.find('</text>')\n",
    "    text = contents[text_start+len('<text>'):text_end]\n",
    "    return title+\" \"+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = get_stopwords()\n",
    "    filtered_words = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(data):\n",
    "    tokens = []\n",
    "    for token_list in data.values():\n",
    "        tokens = tokens + token_list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    return list(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(contents):\n",
    "    dataDict = {}\n",
    "    data_list = []\n",
    "    for content in contents:\n",
    "\n",
    "        tokens = tokenize_and_remove_punctuations(content)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "\n",
    "        if len(stemmed_tokens) !=  0:\n",
    "            data_list.append(stemmed_tokens)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    curr_file_num = 1\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        # print (filename)\n",
    "        if curr_file_num%1000 == 0:\n",
    "            print (curr_file_num)\n",
    "\n",
    "        global file_contents_list\n",
    "\n",
    "        file_data = fileContents()\n",
    "        # contents = []\n",
    "        \n",
    "        data = parse_data(open(path + '/' + filename,'r').read())\n",
    "    \n",
    "        data = data.replace('\\n', ' ').replace(\"'''\", '').replace(\"''\", '').replace(\",\", ' ').strip()\n",
    "\n",
    "        # print (data)\n",
    "        file_data.contents_raw = data\n",
    "        file_data.file_num = curr_file_num\n",
    "        file_data.file_name = filename\n",
    "\n",
    "        # filename = re.sub('\\D',\"\",filename)\n",
    "        # contents.append([file_name_number_mapping_dict[filename],data])\n",
    "        # print (data)\n",
    "    # return contents\n",
    "\n",
    "        # print (type(data), data)\n",
    "        data_list = data.split()\n",
    "        file_data.contents_tokens = preprocess_data(data_list)\n",
    "\n",
    "        file_contents_list.append(file_data)\n",
    "\n",
    "        # inverted_index = generate_inverted_index(preprocess_data_dict)\n",
    "        curr_file_num += 1\n",
    "    return  inverted_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_vec(queries):\n",
    "    global query_to_vec_dict\n",
    "\n",
    "    for key, val in queries.items():\n",
    "        # query_to_vec_dict[key] = []\n",
    "        val_set = set(val)\n",
    "        for tok in val_set:\n",
    "            count = val.count(tok)\n",
    "            query_to_vec_dict[tok] = count\n",
    "\n",
    "    # print(key, val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_queries(path):\n",
    "    queriesDict = {}\n",
    "    queries = open(path,'r').read().split('\\n')\n",
    "    i = 1\n",
    "    for query in queries:\n",
    "        tokens = tokenize_and_remove_punctuations(query)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "        filtered_tokens1 = remove_stop_words(stemmed_tokens)\n",
    "        queriesDict[i] = filtered_tokens1\n",
    "        i+=1\n",
    "    return queriesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentences(contents):\n",
    "    # dataDict = {}\n",
    "    final_tokens = []\n",
    "    \n",
    "    for content in contents:\n",
    "        tokens = tokenize_and_remove_punctuations(content)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "        filtered_tokens1 = remove_stop_words(stemmed_tokens)\n",
    "        # dataDict[content[0]] = filtered_tokens1\n",
    "        \n",
    "        \n",
    "        for tok in filtered_tokens1:\n",
    "            # print (len(tok), tok)\n",
    "            if len(tok) != 0:\n",
    "                final_tokens.append(tok)\n",
    "\n",
    "    # print (final_tokens)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read tokensied data from file \n",
    "with open(file_name, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    content = line.split(\"!@#\")\n",
    "    file_content = fileContents()\n",
    "    file_content.file_name = content[0]\n",
    "    file_content.file_number = int(content[1])\n",
    "    file_content.contents_raw = content[2]\n",
    "    file_content.contents_tokens = content[3].replace(\"'], ['\", \",\").replace(\"\\n\", \"\").strip(\"'[]'\").split(',')\n",
    "\n",
    "    file_contents_list.append(file_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverted index\n",
    "\n",
    "with open(inverted_index_file, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    content = line.split(\"\\t\")\n",
    "    inverted_index_list = content[1].strip('\\n').strip(\"[]\").replace(\"], [\", \":\").split(':')\n",
    "    inverted_index_list  = [x.split(',') for x in inverted_index_list]\n",
    "    \n",
    "    for pair in inverted_index_list:\n",
    "        pair[0] = int(pair[0])\n",
    "        pair[1] = int(pair[1].strip())\n",
    "\n",
    "    inverted_index[content[0]] = inverted_index_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadQuery():\n",
    "    global query_to_vec_dict\n",
    "    query_to_vec_dict = {}\n",
    "\n",
    "    queries_dict = preprocess_queries('queries.txt')\n",
    "    # print (queries_dict)\n",
    "    query_to_vec(queries_dict)\n",
    "\n",
    "#     print (\"Tokenised query along with frequency\")\n",
    "#     for key, val in query_to_vec_dict.items():\n",
    "#         print (key, val)\n",
    "\n",
    "    return query_to_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexForQuery(query_to_vec_dict):\n",
    "\n",
    "    index_for_query_dict = {}\n",
    "    index_for_query_list = []\n",
    "\n",
    "    for tok in query_to_vec_dict:\n",
    "\n",
    "        if tok in inverted_index.keys():\n",
    "            doc_list = inverted_index[tok]\n",
    "            len_doc_list = len(doc_list)\n",
    "\n",
    "            for pair in doc_list:\n",
    "                pair[1] = pair[1]/(len_doc_list)\n",
    "                \n",
    "                # print (type(pair[0]))  \n",
    "\n",
    "            for doc in doc_list:\n",
    "                if doc[0] in index_for_query_dict.keys():\n",
    "                    index_for_query_dict[doc[0]] += doc[1]\n",
    "                else:\n",
    "                    index_for_query_dict[doc[0]] = doc[1]\n",
    "\n",
    "    \n",
    "    for key, val in index_for_query_dict.items():\n",
    "        index_for_query_list.append([key, val])\n",
    "    \n",
    "    index_for_query_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    index_for_query_list = index_for_query_list[:25]\n",
    "\n",
    "#     print()\n",
    "#     print (\"Index retrieved for query\")\n",
    "#     print ((index_for_query_list))\n",
    "\n",
    "    return index_for_query_list\n",
    "# print (len(index_for_query[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentences(index_for_query_list):\n",
    "    sentences_list = []\n",
    "\n",
    "    global file_contents_list\n",
    "\n",
    "#     for entry in index_for_query_list: \n",
    "#         sentences = readFileOnly(entry[0])\n",
    "        \n",
    "#         for line in sentences:\n",
    "#             sentence_match = sentenceMatch()\n",
    "#             sentence_match.sentence_raw = line.strip()\n",
    "\n",
    "#             for content in file_contents_list:\n",
    "#                 if content.file_number == entry[0]:\n",
    "#                     sentence_match.file_name = content.file_name\n",
    "\n",
    "#             sentence_match.sentence_tokenised = processSentences(sentence_match.sentence_raw.split())\n",
    "#             sentences_list.append(sentence_match)\n",
    "    \n",
    "    # for entry in sentences_list:\n",
    "    #     print (entry.fil\n",
    "    for entry in index_for_query_list: \n",
    "        try:\n",
    "            sentences = readFileOnly(entry[0])\n",
    "\n",
    "            for line in sentences:\n",
    "                sentence_match = sentenceMatch()\n",
    "                sentence_match.sentence_raw = line.strip()\n",
    "\n",
    "                for content in file_contents_list:\n",
    "                    if content.file_number == entry[0]:\n",
    "                        sentence_match.file_name = content.file_name\n",
    "\n",
    "                # print (sentence_match.file_name, sentence_match.sentence_raw)\n",
    "                sentence_match.sentence_tokenised = processSentences(sentence_match.sentence_raw.split())\n",
    "                # print (type(sentence_match.sentence_tokenised), sentence_match.sentence_tokenised)\n",
    "                # processSentences(sentence_match.sentence_raw.split())\n",
    "                sentences_list.append(sentence_match)\n",
    "        except:\n",
    "            print(\"Wait for a while!\")\n",
    "\n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSentences(query_to_vec_dict, sentences_list):\n",
    "    query_token = []\n",
    "\n",
    "    for key in query_to_vec_dict:\n",
    "        query_token.append(key)\n",
    "\n",
    "    # print (query_token)\n",
    "\n",
    "    for entry in sentences_list:\n",
    "        match_count = 0\n",
    "        for tok in query_token:\n",
    "            if tok in entry.sentence_tokenised:\n",
    "                match_count += 1*query_to_vec_dict[tok]\n",
    "        entry.match = match_count\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopSentences(sentences_list):\n",
    "    res = sorted(sentences_list, key = lambda sentenceMatch : sentenceMatch.match, reverse=True)\n",
    "\n",
    "    res = res[:10]\n",
    "\n",
    "    print (\"The obtained sentences are:\")\n",
    "    for entry in res:\n",
    "        print (entry.file_name, entry.sentence_raw)\n",
    "        print ()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchResults(search_query):\n",
    "    f = open(\"queries.txt\", \"w\")\n",
    "    f.write(search_query)\n",
    "    f.close()\n",
    "    query_dict = loadQuery()\n",
    "    index_for_query_list = getIndexForQuery(query_dict)\n",
    "    sentences = extractSentences(index_for_query_list)\n",
    "    rankSentences(query_dict, sentences)\n",
    "    printTopSentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Here....\n",
      "A week has seven days\n",
      "\n",
      "\n",
      "\n",
      "The obtained sentences are:\n",
      "April.txt April always begins on the same day of week as July and additionally January in leap years\n",
      "\n",
      "April.txt April always ends on the same day of the week as December\n",
      "\n",
      "April.txt April begins on the same day of the week as July every year and on the same day of the week as January in leap years\n",
      "\n",
      "April.txt April ends on the same day of the week as December every year as each other's last days are exactly 35 weeks (245 days) apart\n",
      "\n",
      "April.txt In common years April starts on the same day of the week as October of the previous year and in leap years May of the previous year\n",
      "\n",
      "April.txt In common years April finishes on the same day of the week as July of the previous year and in leap years February and October of the previous year\n",
      "\n",
      "April.txt In common years immediately after other common years April starts on the same day of the week as January of the previous year and in leap years and years immediately after that April finishes on the same day of the week as January of the previous year\n",
      "\n",
      "April.txt In years immediately before common years April starts on the same day of the week as September and December of the following year and in years immediately before leap years June of the following year\n",
      "\n",
      "April.txt In years immediately before common years April finishes on the same day of the week as September of the following year and in years immediately before leap years March and June of the following year\n",
      "\n",
      "February.txt February begins on the same day of the week as March and November in common years and August in leap years\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = input('Search Here....\\n')\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "searchResults(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
